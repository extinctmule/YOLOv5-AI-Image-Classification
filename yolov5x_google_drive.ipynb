{"cells":[{"cell_type":"markdown","source":["# Step 1"],"metadata":{"id":"jxl1JF835EKn"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DxsxnCK4J_II","executionInfo":{"status":"ok","timestamp":1669624444387,"user_tz":-540,"elapsed":23853,"user":{"displayName":"ÏûÑÏàòÎØº","userId":"11180160156963732282"}},"outputId":"1157a7ab-99f9-4573-8324-c4ebbd38038b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import torch\n","import os\n","from IPython.display import Image, clear_output  # to display images"],"metadata":{"id":"nfLPzSrPQ73O","executionInfo":{"status":"ok","timestamp":1669624448600,"user_tz":-540,"elapsed":2143,"user":{"displayName":"ÏûÑÏàòÎØº","userId":"11180160156963732282"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["\n","torch.cuda.is_available()\n","print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"],"metadata":{"id":"yMgO4TB9Ysot","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669624450952,"user_tz":-540,"elapsed":520,"user":{"displayName":"ÏûÑÏàòÎØº","userId":"11180160156963732282"}},"outputId":"9248e594-eaf5-40b4-a9ce-e7d46e512a23"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Setup complete. Using torch 1.12.1+cu113 (Tesla T4)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WJ_dtykZu2u_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668738528918,"user_tz":-540,"elapsed":631,"user":{"displayName":"ÏûÑÏàòÎØº","userId":"11180160156963732282"}},"outputId":"c344d291-4513-47d1-823d-ab5f36c8d2fa"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/yolov5x/yolov5\n"]}],"source":["%cd /content/drive/MyDrive/yolov5x/yolov5\n","# %pip install -r requirements.txt\n"]},{"cell_type":"markdown","metadata":{"id":"X7yAi9hd-T4B"},"source":["# Run train.py\n","\n","Here, we are able to pass a number of arguments:\n","- **img:** define input image size\n","- **batch:** determine batch size\n","- **epochs:** define the number of training epochs. (Note: often, 3000+ are common here!)\n","- **data:** Our dataset locaiton is saved in the `dataset.location`\n","- **weights:** specify a path to weights to start transfer learning from. Here we choose the generic COCO pretrained checkpoint.\n","- **cache:** cache images for faster training"]},{"cell_type":"markdown","metadata":{"id":"Cv6L2K-G6xLh"},"source":["# train, testÏö© dataset ÎÇòÎàÑÍ∏∞"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":367,"status":"ok","timestamp":1669624457235,"user":{"displayName":"ÏûÑÏàòÎØº","userId":"11180160156963732282"},"user_tz":-540},"id":"NhC9KATJ07SS","colab":{"base_uri":"https://localhost:8080/"},"outputId":"aaf452dd-26a2-499f-c795-6f3f88b20150"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/yolov5x/yolov5\n"]}],"source":["%cd /content/drive/MyDrive/yolov5x/yolov5\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31180,"status":"ok","timestamp":1669624490042,"user":{"displayName":"ÏûÑÏàòÎØº","userId":"11180160156963732282"},"user_tz":-540},"id":"DRqtkRMw3l6u","outputId":"44a1d398-4071-4728-ac48-91956738c221"},"outputs":[{"output_type":"stream","name":"stdout","text":["2207\n"]}],"source":["\n","from glob import glob\n","\n","img_list = glob('/content/drive/MyDrive/yolov5x/yolov5/dataset_categories/images/*.jpg')\n","print(len(img_list))"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":857,"status":"ok","timestamp":1669624519524,"user":{"displayName":"ÏûÑÏàòÎØº","userId":"11180160156963732282"},"user_tz":-540},"id":"S4sVa1hq65Kc","outputId":"1083f2f2-f4de-46f2-d42b-8e76c343480d"},"outputs":[{"output_type":"stream","name":"stdout","text":["1986 221\n"]}],"source":["\n","from sklearn.model_selection import train_test_split\n","train_img_list, test_img_list = train_test_split(img_list, test_size=0.1, random_state=2000)\n","print(len(train_img_list), len(test_img_list))\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"CKw8lp5rEpnD","executionInfo":{"status":"ok","timestamp":1669624744942,"user_tz":-540,"elapsed":472,"user":{"displayName":"ÏûÑÏàòÎØº","userId":"11180160156963732282"}}},"outputs":[],"source":["with open('/content/drive/MyDrive/yolov5x/yolov5/dataset_categories/images/train.txt', 'w') as f:\n","    f.write('\\n'.join(train_img_list) + '\\n')\n","with open('/content/drive/MyDrive/yolov5x/yolov5/dataset_categories/images/test.txt', 'w') as f:\n","    f.write('\\n'.join(test_img_list) + '\\n')"]},{"cell_type":"code","source":["!git pull"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"agZAgYH8nqxg","executionInfo":{"status":"ok","timestamp":1669624847459,"user_tz":-540,"elapsed":59862,"user":{"displayName":"ÏûÑÏàòÎØº","userId":"11180160156963732282"}},"outputId":"16e09588-43cf-4775-e165-f890727313b3"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["remote: Enumerating objects: 448, done.\u001b[K\n","remote: Counting objects:   0% (1/349)\u001b[K\rremote: Counting objects:   1% (4/349)\u001b[K\rremote: Counting objects:   2% (7/349)\u001b[K\rremote: Counting objects:   3% (11/349)\u001b[K\rremote: Counting objects:   4% (14/349)\u001b[K\rremote: Counting objects:   5% (18/349)\u001b[K\rremote: Counting objects:   6% (21/349)\u001b[K\rremote: Counting objects:   7% (25/349)\u001b[K\rremote: Counting objects:   8% (28/349)\u001b[K\rremote: Counting objects:   9% (32/349)\u001b[K\rremote: Counting objects:  10% (35/349)\u001b[K\rremote: Counting objects:  11% (39/349)\u001b[K\rremote: Counting objects:  12% (42/349)\u001b[K\rremote: Counting objects:  13% (46/349)\u001b[K\rremote: Counting objects:  14% (49/349)\u001b[K\rremote: Counting objects:  15% (53/349)\u001b[K\rremote: Counting objects:  16% (56/349)\u001b[K\rremote: Counting objects:  17% (60/349)\u001b[K\rremote: Counting objects:  18% (63/349)\u001b[K\rremote: Counting objects:  19% (67/349)\u001b[K\rremote: Counting objects:  20% (70/349)\u001b[K\rremote: Counting objects:  21% (74/349)\u001b[K\rremote: Counting objects:  22% (77/349)\u001b[K\rremote: Counting objects:  23% (81/349)\u001b[K\rremote: Counting objects:  24% (84/349)\u001b[K\rremote: Counting objects:  25% (88/349)\u001b[K\rremote: Counting objects:  26% (91/349)\u001b[K\rremote: Counting objects:  27% (95/349)\u001b[K\rremote: Counting objects:  28% (98/349)\u001b[K\rremote: Counting objects:  29% (102/349)\u001b[K\rremote: Counting objects:  30% (105/349)\u001b[K\rremote: Counting objects:  31% (109/349)\u001b[K\rremote: Counting objects:  32% (112/349)\u001b[K\rremote: Counting objects:  33% (116/349)\u001b[K\rremote: Counting objects:  34% (119/349)\u001b[K\rremote: Counting objects:  35% (123/349)\u001b[K\rremote: Counting objects:  36% (126/349)\u001b[K\rremote: Counting objects:  37% (130/349)\u001b[K\rremote: Counting objects:  38% (133/349)\u001b[K\rremote: Counting objects:  39% (137/349)\u001b[K\rremote: Counting objects:  40% (140/349)\u001b[K\rremote: Counting objects:  41% (144/349)\u001b[K\rremote: Counting objects:  42% (147/349)\u001b[K\rremote: Counting objects:  43% (151/349)\u001b[K\rremote: Counting objects:  44% (154/349)\u001b[K\rremote: Counting objects:  45% (158/349)\u001b[K\rremote: Counting objects:  46% (161/349)\u001b[K\rremote: Counting objects:  47% (165/349)\u001b[K\rremote: Counting objects:  48% (168/349)\u001b[K\rremote: Counting objects:  49% (172/349)\u001b[K\rremote: Counting objects:  50% (175/349)\u001b[K\rremote: Counting objects:  51% (178/349)\u001b[K\rremote: Counting objects:  52% (182/349)\u001b[K\rremote: Counting objects:  53% (185/349)\u001b[K\rremote: Counting objects:  54% (189/349)\u001b[K\rremote: Counting objects:  55% (192/349)\u001b[K\rremote: Counting objects:  56% (196/349)\u001b[K\rremote: Counting objects:  57% (199/349)\u001b[K\rremote: Counting objects:  58% (203/349)\u001b[K\rremote: Counting objects:  59% (206/349)\u001b[K\rremote: Counting objects:  60% (210/349)\u001b[K\rremote: Counting objects:  61% (213/349)\u001b[K\rremote: Counting objects:  62% (217/349)\u001b[K\rremote: Counting objects:  63% (220/349)\u001b[K\rremote: Counting objects:  64% (224/349)\u001b[K\rremote: Counting objects:  65% (227/349)\u001b[K\rremote: Counting objects:  66% (231/349)\u001b[K\rremote: Counting objects:  67% (234/349)\u001b[K\rremote: Counting objects:  68% (238/349)\u001b[K\rremote: Counting objects:  69% (241/349)\u001b[K\rremote: Counting objects:  70% (245/349)\u001b[K\rremote: Counting objects:  71% (248/349)\u001b[K\rremote: Counting objects:  72% (252/349)\u001b[K\rremote: Counting objects:  73% (255/349)\u001b[K\rremote: Counting objects:  74% (259/349)\u001b[K\rremote: Counting objects:  75% (262/349)\u001b[K\rremote: Counting objects:  76% (266/349)\u001b[K\rremote: Counting objects:  77% (269/349)\u001b[K\rremote: Counting objects:  78% (273/349)\u001b[K\rremote: Counting objects:  79% (276/349)\u001b[K\rremote: Counting objects:  80% (280/349)\u001b[K\rremote: Counting objects:  81% (283/349)\u001b[K\rremote: Counting objects:  82% (287/349)\u001b[K\rremote: Counting objects:  83% (290/349)\u001b[K\rremote: Counting objects:  84% (294/349)\u001b[K\rremote: Counting objects:  85% (297/349)\u001b[K\rremote: Counting objects:  86% (301/349)\u001b[K\rremote: Counting objects:  87% (304/349)\u001b[K\rremote: Counting objects:  88% (308/349)\u001b[K\rremote: Counting objects:  89% (311/349)\u001b[K\rremote: Counting objects:  90% (315/349)\u001b[K\rremote: Counting objects:  91% (318/349)\u001b[K\rremote: Counting objects:  92% (322/349)\u001b[K\rremote: Counting objects:  93% (325/349)\u001b[K\rremote: Counting objects:  94% (329/349)\u001b[K\rremote: Counting objects:  95% (332/349)\u001b[K\rremote: Counting objects:  96% (336/349)\u001b[K\rremote: Counting objects:  97% (339/349)\u001b[K\rremote: Counting objects:  98% (343/349)\u001b[K\rremote: Counting objects:  99% (346/349)\u001b[K\rremote: Counting objects: 100% (349/349)\u001b[K\rremote: Counting objects: 100% (349/349), done.\u001b[K\n","remote: Compressing objects: 100% (60/60), done.\u001b[K\n","remote: Total 448 (delta 302), reused 321 (delta 289), pack-reused 99\u001b[K\n","Receiving objects: 100% (448/448), 222.02 KiB | 222.00 KiB/s, done.\n","Resolving deltas: 100% (326/326), completed with 75 local objects.\n","From https://github.com/ultralytics/yolov5\n","   ff6e6e3..350e8eb  master          -> origin/master\n"," * [new branch]      exp12           -> origin/exp12\n"," * [new branch]      exp13           -> origin/exp13\n","   aa723e4..2e3c515  ultralytics/HUB -> origin/ultralytics/HUB\n","   3d1b364..3d89282  update/threaded -> origin/update/threaded\n"," * [new tag]         v7.0            -> v7.0\n","Updating 8a19437..350e8eb\n","error: Your local changes to the following files would be overwritten by merge:\n","\t.github/README_cn.md\n","\t.github/workflows/ci-testing.yml\n","\t.pre-commit-config.yaml\n","\tREADME.md\n","\tbenchmarks.py\n","\tclassify/predict.py\n","\tclassify/train.py\n","\tclassify/val.py\n","\tdata/Argoverse.yaml\n","\tdata/scripts/download_weights.sh\n","\tdata/scripts/get_coco.sh\n","\tdata/xView.yaml\n","\tdetect.py\n","\texport.py\n","\thubconf.py\n","\tmodels/common.py\n","\tmodels/tf.py\n","\trequirements.txt\n","\tsegment/predict.py\n","\tsegment/train.py\n","\tsegment/val.py\n","\ttrain.py\n","\ttutorial.ipynb\n","\tutils/augmentations.py\n","\tutils/autoanchor.py\n","\tutils/dataloaders.py\n","\tutils/docker/Dockerfile\n","\tutils/docker/Dockerfile-arm64\n","\tutils/docker/Dockerfile-cpu\n","\tutils/downloads.py\n","\tutils/general.py\n","\tutils/loggers/__init__.py\n","\tutils/loggers/clearml/README.md\n","\tutils/loggers/clearml/clearml_utils.py\n","\tutils/loggers/comet/README.md\n","\tutils/loggers/comet/__init__.py\n","\tutils/loggers/wandb/wandb_utils.py\n","\tutils/metrics.py\n","\tutils/segment/dataloaders.py\n","\tutils/segment/general.py\n","\tutils/torch_utils.py\n","\tval.py\n","Please commit your changes or stash them before you merge.\n","Aborting\n"]}]},{"cell_type":"code","execution_count":13,"metadata":{"id":"peQ7AyOqPzgy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2620d9d1-56ee-4042-e6a5-51233f7934df","executionInfo":{"status":"ok","timestamp":1669628820159,"user_tz":-540,"elapsed":3314705,"user":{"displayName":"ÏûÑÏàòÎØº","userId":"11180160156963732282"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5x.pt, cfg=yolov5x_categories.yaml, data=data_categories.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=100, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=disk, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n","\u001b[34m\u001b[1mgithub: \u001b[0m‚ö†Ô∏è YOLOv5 is out of date by 95 commits. Use `git pull` or `git clone https://github.com/ultralytics/yolov5` to update.\n","YOLOv5 üöÄ v6.2-181-g8a19437 Python-3.7.15 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n","\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 üöÄ runs in Weights & Biases\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 üöÄ in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      8800  models.common.Conv                      [3, 80, 6, 2, 2]              \n","  1                -1  1    115520  models.common.Conv                      [80, 160, 3, 2]               \n","  2                -1  4    309120  models.common.C3                        [160, 160, 4]                 \n","  3                -1  1    461440  models.common.Conv                      [160, 320, 3, 2]              \n","  4                -1  8   2259200  models.common.C3                        [320, 320, 8]                 \n","  5                -1  1   1844480  models.common.Conv                      [320, 640, 3, 2]              \n","  6                -1 12  13125120  models.common.C3                        [640, 640, 12]                \n","  7                -1  1   7375360  models.common.Conv                      [640, 1280, 3, 2]             \n","  8                -1  4  19676160  models.common.C3                        [1280, 1280, 4]               \n","  9                -1  1   4099840  models.common.SPPF                      [1280, 1280, 5]               \n"," 10                -1  1    820480  models.common.Conv                      [1280, 640, 1, 1]             \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  4   5332480  models.common.C3                        [1280, 640, 4, False]         \n"," 14                -1  1    205440  models.common.Conv                      [640, 320, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  4   1335040  models.common.C3                        [640, 320, 4, False]          \n"," 18                -1  1    922240  models.common.Conv                      [320, 320, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  4   4922880  models.common.C3                        [640, 640, 4, False]          \n"," 21                -1  1   3687680  models.common.Conv                      [640, 640, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  4  19676160  models.common.C3                        [1280, 1280, 4, False]        \n"," 24      [17, 20, 23]  1    134580  models.yolo.Detect                      [15, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [320, 640, 1280]]\n","YOLOv5x_categories summary: 445 layers, 86312020 parameters, 86312020 gradients, 204.9 GFLOPs\n","\n","Transferred 738/745 items from yolov5x.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 123 weight(decay=0.0), 126 weight(decay=0.0005), 126 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/drive/MyDrive/yolov5x/yolov5/dataset_categories/labels.cache' images and labels... 2207 found, 0 missing, 0 empty, 0 corrupt: 100% 2207/2207 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (2.7GB disk): 100% 2207/2207 [00:00<00:00, 3357.34it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/drive/MyDrive/yolov5x/yolov5/dataset_categories/labels.cache' images and labels... 2207 found, 0 missing, 0 empty, 0 corrupt: 100% 2207/2207 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mCaching images (2.7GB disk): 100% 2207/2207 [00:00<00:00, 3412.92it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.97 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ‚úÖ\n","Plotting labels to runs/train/exp2/labels.jpg... \n","Image sizes 640 train, 640 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/train/exp2\u001b[0m\n","Starting training for 100 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       0/99      13.8G    0.07149    0.07866    0.07039        216        640: 100% 138/138 [02:57<00:00,  1.29s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 69/69 [01:13<00:00,  1.06s/it]\n","                   all       2207      21943     0.0729      0.533     0.0846     0.0374\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       1/99        14G     0.0541    0.05712    0.06684        223        640: 100% 138/138 [02:52<00:00,  1.25s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 69/69 [01:11<00:00,  1.03s/it]\n","                   all       2207      21943      0.208      0.304      0.136     0.0587\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       2/99        14G    0.04892     0.0529    0.06486        251        640: 100% 138/138 [02:51<00:00,  1.24s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 69/69 [01:10<00:00,  1.03s/it]\n","                   all       2207      21943      0.353      0.312      0.201     0.0998\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       3/99        14G    0.04183     0.0507     0.0606        301        640: 100% 138/138 [02:50<00:00,  1.24s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 69/69 [01:10<00:00,  1.02s/it]\n","                   all       2207      21943      0.435      0.414      0.291      0.157\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       4/99        14G    0.03734    0.04925    0.05663        231        640: 100% 138/138 [02:50<00:00,  1.24s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 69/69 [01:10<00:00,  1.02s/it]\n","                   all       2207      21943      0.447      0.476      0.333      0.168\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       5/99        14G    0.03379    0.04851    0.05389        172        640: 100% 138/138 [02:50<00:00,  1.24s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 69/69 [01:10<00:00,  1.02s/it]\n","                   all       2207      21943      0.434       0.52      0.397      0.217\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       6/99        14G    0.03257    0.04825    0.05129        246        640: 100% 138/138 [02:50<00:00,  1.24s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 69/69 [01:11<00:00,  1.03s/it]\n","                   all       2207      21943      0.433      0.543       0.42      0.269\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       7/99        14G    0.03132    0.04689    0.04888        200        640: 100% 138/138 [02:51<00:00,  1.24s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 69/69 [01:10<00:00,  1.03s/it]\n","                   all       2207      21943      0.517      0.572      0.505      0.301\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       8/99        14G    0.02985    0.04656    0.04671        214        640: 100% 138/138 [02:50<00:00,  1.24s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 69/69 [01:10<00:00,  1.03s/it]\n","                   all       2207      21943      0.595      0.543      0.536      0.339\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       9/99        14G    0.02953    0.04681     0.0441        197        640: 100% 138/138 [02:50<00:00,  1.24s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 69/69 [01:10<00:00,  1.02s/it]\n","                   all       2207      21943      0.656      0.582      0.608      0.415\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      10/99        14G    0.02856    0.04488    0.04269        225        640: 100% 138/138 [02:50<00:00,  1.24s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 69/69 [01:10<00:00,  1.03s/it]\n","                   all       2207      21943      0.624       0.56      0.579      0.408\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      11/99        14G    0.02768    0.04555    0.04147        250        640: 100% 138/138 [02:50<00:00,  1.23s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 69/69 [01:11<00:00,  1.03s/it]\n","                   all       2207      21943      0.679      0.603      0.632      0.428\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      12/99        14G    0.02744    0.04483    0.04005        218        640: 100% 138/138 [02:50<00:00,  1.24s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 69/69 [01:10<00:00,  1.03s/it]\n","                   all       2207      21943       0.68      0.618      0.633      0.429\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      13/99        14G    0.02736    0.04564    0.03981        281        640:  15% 21/138 [00:26<02:26,  1.25s/it]\n","Traceback (most recent call last):\n","  File \"train.py\", line 643, in <module>\n","    main(opt)\n","  File \"train.py\", line 539, in main\n","    train(opt.hyp, opt, device, callbacks)\n","  File \"train.py\", line 321, in train\n","    loss, loss_items = compute_loss(pred, targets.to(device))  # loss scaled by batch_size\n","KeyboardInterrupt\n"]}],"source":["# !python train.py --resume 'runs/train/exp5/weights/last.pt'\n","!python train.py --img 640 --batch 16 --epochs 100 --data data_categories.yaml --weights yolov5x.pt --cfg yolov5x_categories.yaml --cache disk\n"]},{"cell_type":"markdown","source":["# PrecisionÍ≥º Recall Í¥ÄÎ†®\n","PrecisionÍ≥º RecallÏùÄ Ìï≠ÏÉÅ 0Í≥º 1ÏÇ¨Ïù¥Ïùò Í∞íÏúºÎ°ú ÎÇòÏò§Í≤å ÎêòÎäîÎç∞, PrecisionÏù¥ ÎÜíÏúºÎ©¥ RecallÏùÄ ÎÇÆÏùÄ Í≤ΩÌñ•Ïù¥ ÏûàÍ≥†, PrecisionÏù¥ ÎÇÆÏúºÎ©¥ RecallÏù¥ ÎÜíÏùÄ Í≤ΩÌñ•Ïù¥ ÏûàÎã§Îäî Í≤ÉÏù¥Îã§. Îî∞ÎùºÏÑú Ïñ¥Îäê Ìïú Í∞í ÎßåÏúºÎ°ú ÏïåÍ≥†Î¶¨Ï¶òÏùò ÏÑ±Îä•ÏùÑ ÌèâÍ∞ÄÌïòÎäî Í≤ÉÏùÄ Í±∞Ïùò Î∂àÍ∞ÄÎä•ÌïòÍ≥†, Îëê Í∞íÏùÑ Ï¢ÖÌï©Ìï¥ÏÑú ÏïåÍ≥†Î¶¨Ï¶òÏùò ÏÑ±Îä•ÏùÑ ÌèâÍ∞ÄÌï¥Ïïº ÌïúÎã§. Í∑∏ÎûòÏÑú ÌïÑÏöîÌïú Í≤ÉÏù¥ precision-recall Í≥°ÏÑ† Î∞è APÏù¥Îã§. \n","\n","TP: true positive\n","mAP: mean Average Precision\n","\n","https://bskyvision.com/m/465"],"metadata":{"id":"FdmXIK247lTd"}},{"cell_type":"markdown","metadata":{"id":"jtmS7_TXFsT3"},"source":["## Validate With Trained Weights"]},{"cell_type":"code","source":["#!python train.py --resume runs/train/exp3/weights/last.pt --weights runs/train/colors_yolov5x_results37/weights/best.pt\n","\n","%cd /content/drive/MyDrive/yolov5x/yolov5"],"metadata":{"id":"LvJEnjE_zKpC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668733883582,"user_tz":-540,"elapsed":6,"user":{"displayName":"ÏûÑÏàòÎØº","userId":"11180160156963732282"}},"outputId":"909d1dab-e7d4-43e8-d5da-46b49615bc81"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/yolov5x/yolov5\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZbUn4_b9GCKO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668734042280,"user_tz":-540,"elapsed":156469,"user":{"displayName":"ÏûÑÏàòÎØº","userId":"11180160156963732282"}},"outputId":"d67a5db7-9159-4648-e79d-fdf69689f3a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mval: \u001b[0mdata=data_colors.yaml, weights=['runs/train/exp4/weights/best.pt'], batch_size=16, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=test, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=True, save_hybrid=False, save_conf=True, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n","\u001b[31m\u001b[1mrequirements:\u001b[0m YOLOv5 requirement \"ipython\" not found, attempting AutoUpdate...\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (7.9.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython) (0.7.5)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython) (57.4.0)\n","Collecting jedi>=0.10\n","  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython) (2.6.1)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython) (4.8.0)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython) (0.2.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython) (4.4.2)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython) (5.1.1)\n","Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython) (2.0.10)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython) (0.8.3)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython) (0.2.5)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython) (1.15.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython) (0.7.0)\n","Installing collected packages: jedi\n","Successfully installed jedi-0.18.1\n","\n","\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per /content/drive/MyDrive/yolov5x/yolov5/requirements.txt\n","\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ö†Ô∏è \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n","\n","YOLOv5 üöÄ v6.2-181-g8a19437 Python-3.7.15 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n","\n","Fusing layers... \n","YOLOv5x summary: 322 layers, 86254162 parameters, 0 gradients\n","Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n","100% 755k/755k [00:00<00:00, 123MB/s]\n","\u001b[34m\u001b[1mtest: \u001b[0mScanning '/content/drive/MyDrive/yolov5x/yolov5/dataset_colors/test/labels.cache' images and labels... 144 found, 0 missing, 0 empty, 0 corrupt: 100% 144/144 [00:00<?, ?it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 9/9 [01:01<00:00,  6.80s/it]\n","                   all        144       1215      0.812      0.806      0.852      0.692\n","                 multi        144         63       0.61      0.508      0.585      0.472\n","                   red        144        153      0.941      0.961      0.985      0.808\n","                orange        144         57      0.687      0.656      0.735      0.497\n","                yellow        144         76       0.73      0.934      0.855      0.705\n","                  nude        144        102       0.71      0.637      0.764      0.599\n","                  pink        144         68      0.839      0.838      0.885      0.717\n","                 green        144         83      0.902      0.819      0.887      0.747\n","               skyblue        144         98      0.931      0.898      0.955      0.783\n","                  navy        144        108      0.893      0.944      0.964       0.81\n","                purple        144         75       0.95       0.84      0.925      0.744\n","                 black        144         77      0.848      0.922      0.927      0.773\n","                 white        144        122      0.758      0.738      0.785      0.659\n","                silver        144        133      0.752      0.782      0.819      0.686\n","Speed: 0.4ms pre-process, 54.6ms inference, 2.5ms NMS per image at shape (16, 3, 640, 640)\n","Results saved to \u001b[1mruns/val/exp15\u001b[0m\n","144 labels saved to runs/val/exp15/labels\n"]}],"source":["!python val.py --weights runs/train/exp7/weights/best.pt --data data_colors.yaml --task test --img 640 --batch-size 16 --save-txt --save-conf"]},{"cell_type":"markdown","metadata":{"id":"AcIRLQOlA14A"},"source":["## inference\n","\n"]},{"cell_type":"code","source":["%pip install -r requirements.txt\n","\n","import torch\n","import os\n","from IPython.display import Image, clear_output  # to display images\n","\n","torch.cuda.is_available()\n","print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"],"metadata":{"id":"eIpDcCCzOCz3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/yolov5x/yolov5\n","\n","!python detect.py --data data_colors.yaml --weights runs/train/exp4/weights/best.pt --save-txt --source 'dataset_colors/test/images/1658900587_CggSAW_L2f4.jpg' --img 640\n"],"metadata":{"id":"rWpcDaF3vvWo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668734648689,"user_tz":-540,"elapsed":8758,"user":{"displayName":"ÏûÑÏàòÎØº","userId":"11180160156963732282"}},"outputId":"f09b7dbb-3fa9-4713-b93b-c65133cd8d68"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/yolov5x/yolov5\n","\u001b[34m\u001b[1mdetect: \u001b[0mweights=['runs/train/exp4/weights/best.pt'], source=dataset_colors/test/images/1658900587_CggSAW_L2f4.jpg, data=data_colors.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=True, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n","YOLOv5 üöÄ v6.2-181-g8a19437 Python-3.7.15 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n","\n","Fusing layers... \n","YOLOv5x summary: 322 layers, 86254162 parameters, 0 gradients\n","image 1/1 /content/drive/MyDrive/yolov5x/yolov5/dataset_colors/test/images/1658900587_CggSAW_L2f4.jpg: 640x640 2 multis, 2 reds, 2 yellows, 2 greens, 2 navys, 88.3ms\n","Speed: 0.6ms pre-process, 88.3ms inference, 1.6ms NMS per image at shape (1, 3, 640, 640)\n","Results saved to \u001b[1mruns/detect/exp9\u001b[0m\n","1 labels saved to runs/detect/exp9/labels\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/yolov5x/yolov5\n","!python export.py --weights runs/train/exp3/weights/best.pt --include torchscript onnx"],"metadata":{"id":"rQTG3U6Uhi91","executionInfo":{"status":"ok","timestamp":1667554982996,"user_tz":-540,"elapsed":48043,"user":{"displayName":"ÏûÑÏàòÎØº","userId":"11180160156963732282"}},"outputId":"1cbbf664-dd51-4665-9ff2-00de8d51191f","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/yolov5x/yolov5\n","\u001b[34m\u001b[1mexport: \u001b[0mdata=data/coco128.yaml, weights=['runs/train/exp3/weights/best.pt'], imgsz=[640, 640], batch_size=1, device=cpu, half=False, inplace=False, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=12, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['torchscript', 'onnx']\n","YOLOv5 üöÄ v6.2-181-g8a19437 Python-3.7.15 torch-1.12.1+cu113 CPU\n","\n","Fusing layers... \n","Model summary: 322 layers, 86254162 parameters, 0 gradients, 204.0 GFLOPs\n","\n","\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from runs/train/exp3/weights/best.pt with output shape (1, 25200, 18) (165.3 MB)\n","\n","\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 1.12.1+cu113...\n","\u001b[34m\u001b[1mTorchScript:\u001b[0m export success ‚úÖ 13.8s, saved as runs/train/exp3/weights/best.torchscript (329.8 MB)\n","\u001b[31m\u001b[1mrequirements:\u001b[0m YOLOv5 requirement \"onnx\" not found, attempting AutoUpdate...\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting onnx\n","  Downloading onnx-1.12.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n","Requirement already satisfied: protobuf<=3.20.1,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from onnx) (3.17.3)\n","Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx) (4.1.1)\n","Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnx) (1.21.6)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1,>=3.12.2->onnx) (1.15.0)\n","Installing collected packages: onnx\n","Successfully installed onnx-1.12.0\n","\n","\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per ['onnx']\n","\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ö†Ô∏è \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n","\n","\n","\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.12.0...\n","\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 18.9s, saved as runs/train/exp3/weights/best.onnx (329.5 MB)\n","\n","Export complete (43.5s)\n","Results saved to \u001b[1m/content/drive/MyDrive/yolov5x/yolov5/runs/train/exp3/weights\u001b[0m\n","Detect:          python detect.py --weights runs/train/exp3/weights/best.onnx \n","Validate:        python val.py --weights runs/train/exp3/weights/best.onnx \n","PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'runs/train/exp3/weights/best.onnx')  \n","Visualize:       https://netron.app\n"]}]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","from torchvision.transforms import transforms\n","\n","test_path = '/dataset_colors/test'"],"metadata":{"id":"puL0SZSlMDz8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g8dHcni6CJYt"},"source":["# Conclusion and Next Steps\n","\n","Congratulations! You've trained a custom YOLOv5 model to recognize your custom objects.\n","\n","To improve you model's performance, we recommend first interating on your datasets coverage and quality. See this guide for [model performance improvement](https://github.com/ultralytics/yolov5/wiki/Tips-for-Best-Training-Results).\n","\n","To deploy your model to an application, see this guide on [exporting your model to deployment destinations](https://github.com/ultralytics/yolov5/issues/251).\n","\n","Once your model is in production, you will want to continually iterate and improve on your dataset and model via [active learning](https://blog.roboflow.com/what-is-active-learning/)."]},{"cell_type":"code","source":[],"metadata":{"id":"7rslpj9PBB-d"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":344},"id":"7iiObB2WCMh6","outputId":"172079e2-c9dd-4c56-b58d-eb331f3e6288","executionInfo":{"status":"error","timestamp":1667554494068,"user_tz":-540,"elapsed":1037,"user":{"displayName":"ÏûÑÏàòÎØº","userId":"11180160156963732282"}}},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-92e0a3abb010>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#export your model's weights for future use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'content/drive/MyDrive/yolov5x/yolov5/runs/train/exp3/weights/best.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    207\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Cannot find file: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m   \u001b[0mcomm_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_IPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomm_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: Cannot find file: content/drive/MyDrive/yolov5x/yolov5/runs/train/exp3/weights/best.pt"]}],"source":["\n","#export your model's weights for future use\n","from google.colab import files\n","files.download('content/drive/MyDrive/yolov5x/yolov5/runs/train/exp3/weights/best.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rNn-obvOGITm"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/roboflow-ai/yolov5-custom-training-tutorial/blob/main/yolov5-custom-training.ipynb","timestamp":1665313130452}]},"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"}},"gpuClass":"standard","accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}